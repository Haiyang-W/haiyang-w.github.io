---
permalink: /
title: "About Me"
excerpt: "The main page about me."
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am Haiyang Wang (汪海洋), currently a researcher (TopMinds) at Huawei. I received my Ph.D. from Peking University (PKU) in 2025, advised by [Prof. Liwei Wang](http://www.liweiwang-pku.com/). Prior to that, I obtained my bachelor's degree from Zhiyuan College, Shanghai Jiao Tong University (SJTU), in 2020, supervised by [Prof. Cewu Lu](https://www.mvig.org/). I have also closely collaborated with [Shaoshuai Shi](https://shishaoshuai.com/), and was fortunate to work with [Prof. Bernt Schiele](https://scholar.google.com/citations?user=z76PBfYAAAAJ&hl=en) (MPI-INF), [Prof. Jifeng Dai](https://scholar.google.com/citations?user=SH_-B_AAAAAJ&hl=en) (SenseTime), and [Prof. Wei Wang](https://scholar.google.de/citations?user=UedS9LQAAAAJ&hl=en) (UCLA).

My current research focuses on general-purpose agent systems, with an emphasis on code intelligence and autonomous software engineering. Representative directions include:
* Self-evolving and continually improving agent systems.
* Autonomous program debugging and repair.
* Model architectures tailored for code understanding and generation.
* Closed-loop code agent systems from planning to execution and feedback.

I am currently developing the CodeAgent system toward directly delivering high-quality software services to end users.

**<font color="red">[Hiring] We are seeking both full-time researchers and research interns year-round. We provide strong computational resources and substantial research freedom. If you are interested, please feel free to contact me via email.</font> (wangocean.cs [at] gmail [dot] com)**

Previous research areas:
* Fundamental neural architectures for foundation models.
* Unified computational frameworks for general visual modeling.
* 3D scene understanding for autonomous driving and robotics.
* Embodied AI with reinforcement learning methods.

## Selected Publications
<sub>\* means equal contribution. † indicates corresponding author.</sub>

### Preprints
* [CLI-Gym: Scalable CLI Task Generation via Agentic Environment Inversion](https://arxiv.org/pdf/2602.10999).\\
Yusong Lin, **Haiyang Wang**<sup>†</sup>, Shuzhe Wu, Lue Fan, Feiyang Pan, Sanyuan Zhao<sup>†</sup>, Dandan Tu<sup>†</sup>. \[[Code](https://github.com/LiberCoders/CLI-Gym)\] &nbsp;<a href="https://github.com/LiberCoders/CLI-Gym"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/LiberCoders/CLI-Gym?style=social"></a> &nbsp;\[[Dataset](https://huggingface.co/datasets/LiberCoders/CLI-Gym)\] &nbsp;<a href="https://huggingface.co/datasets/LiberCoders/CLI-Gym"><img alt="Hugging Face downloads" style="vertical-align:middle" src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fhuggingface.co%2Fapi%2Fdatasets%2FLiberCoders%2FCLI-Gym&query=%24.downloads&label=Downloads&suffix=%20monthly&color=white&logo=huggingface&logoColor=FFD21E"></a>

### Publications

* [FeatureBench: Benchmarking Agentic Coding for Complex Feature Development](https://arxiv.org/pdf/2602.10975).\\
Qixing Zhou\*, Jiacheng Zhang\*, **Haiyang Wang\***, Rui Hao, Jiahe Wang, Minghao Han, Yuxue Yang, Shuzhe Wu, Feiyang Pan, Lue Fan<sup>†</sup>, Dandan Tu, Zhaoxiang Zhang<sup>†</sup>. In [**ICLR 2026**](https://iclr.cc/). \[[Code](https://github.com/LiberCoders/FeatureBench)\] &nbsp;<a href="https://github.com/LiberCoders/FeatureBench"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/LiberCoders/FeatureBench?style=social"></a> &nbsp;\[[Dataset](https://huggingface.co/datasets/LiberCoders/FeatureBench)\] &nbsp;<a href="https://huggingface.co/datasets/LiberCoders/FeatureBench"><img alt="Hugging Face downloads" style="vertical-align:middle" src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fhuggingface.co%2Fapi%2Fdatasets%2FLiberCoders%2FFeatureBench&query=%24.downloads&label=Downloads&suffix=%20monthly&color=white&logo=huggingface&logoColor=FFD21E"></a>

* [TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters](https://arxiv.org/abs/2410.23168).\\
**Haiyang Wang\***, Yue Fan\*, Muhammad Ferjad Naeem, Yongqin Xian, Jan Eric Lenssen, Liwei Wang, Federico Tombari, Bernt Schiele. In [**ICLR 2025**](https://iclr.cc/). \[[Code](https://github.com/Haiyang-W/TokenFormer)\] &nbsp;<a href="https://github.com/Haiyang-W/TokenFormer"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/Haiyang-W/TokenFormer?style=social"> </a> **<font color=red>(Spotlight Presentation, 3.20% acceptance rate)</font>**

* [GiT: Towards Generalist Vision Transformer through Universal Language Interface](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04158.pdf).\\
**Haiyang Wang\***, Hao Tang\*, Li Jiang<sup>†</sup>, Shaoshuai Shi, Muhammad Ferjad Naeem, Hongsheng Li, Bernt Schiele, Liwei Wang<sup>†</sup>. In [**ECCV 2024**](https://eccv.ecva.net/Conferences/2024). \[[Code](https://github.com/Haiyang-W/GiT)\] &nbsp;<a href="https://github.com/Haiyang-W/GiT"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/Haiyang-W/GiT?style=social"> </a> **<font color=red>(Oral Presentation, 2.32% acceptance rate)</font>**

* [PRED: Pre-training via Semantic Rendering on LiDAR Point Clouds](https://proceedings.neurips.cc/paper_files/paper/2023/file/903f778fe1341e5351b5b63e0e6b197f-Paper-Conference.pdf).\\
Hao Yang, **Haiyang Wang**, Di Dai, Liwei Wang. In [**NeurIPS 2023**](https://neurips.cc/Conferences/2023).

* [UniTR: A Unified and Efficient Multi-Modal Transformer for Bird's-Eye-View Representation](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_UniTR_A_Unified_and_Efficient_Multi-Modal_Transformer_for_Birds-Eye-View_Representation_ICCV_2023_paper.pdf).\\
**Haiyang Wang\***, Hao Tang\*, Shaoshuai Shi<sup>†</sup>, Aoxue Li, Zhenguo Li, Bernt Schiele, Liwei Wang<sup>†</sup>. In [**ICCV 2023**](https://iccv2023.thecvf.com/). \[[Code](https://github.com/Haiyang-W/UniTR)\] &nbsp;<a href="https://github.com/Haiyang-W/UniTR"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/Haiyang-W/UniTR?style=social"> </a>

* [DSVT: Dynamic Sparse Voxel Transformer with Rotated Sets](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_DSVT_Dynamic_Sparse_Voxel_Transformer_With_Rotated_Sets_CVPR_2023_paper.pdf).\\
**Haiyang Wang\***, Chen Shi\*, Shaoshuai Shi<sup>†</sup>, Meng Li, Sen Wang, Di He, Bernt Schiele, Liwei Wang<sup>†</sup>. In [**CVPR 2023**](https://cvpr.thecvf.com/Conferences/2023). \[[Code](https://github.com/Haiyang-W/DSVT)\] &nbsp;<a href="https://github.com/Haiyang-W/DSVT"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/Haiyang-W/DSVT?style=social"> </a>

* [CAGroup3D: Class-Aware Grouping for 3D Object Detection on Point Clouds](https://proceedings.neurips.cc/paper_files/paper/2022/file/c1aaf7c3f306fe94f77236dc0756d771-Paper-Conference.pdf).\\
**Haiyang Wang\***, Lihe Ding\*, Shaocong Dong, Shaoshuai Shi<sup>†</sup>, Aoxue Li,  Jianan Li,  Zhenguo Li, Liwei Wang<sup>†</sup>. In [**NeurIPS 2022**](https://neurips.cc/Conferences/2022). \[[Code](https://github.com/Haiyang-W/CAGroup3D)\] &nbsp;<a href="https://github.com/Haiyang-W/CAGroup3D"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/Haiyang-W/CAGroup3D?style=social"> </a>

* [MsSVT: Mixed-scale Sparse Voxel Transformer for 3D Object Detection on Point Clouds](https://proceedings.neurips.cc/paper_files/paper/2022/file/4bad7c27534efca029ca0d366c47c0e3-Paper-Conference.pdf).\\
Shaocong Dong\*, Lihe Ding\*, **Haiyang Wang**, Tingfa Xu, Xinli Xu, Jie Wang, Ziyang Bian, Ying Wang, Jianan Li. In [**NeurIPS 2022**](https://neurips.cc/Conferences/2022). \[[Code](https://github.com/dscdyc/MsSVT)\] &nbsp;<a href="https://github.com/dscdyc/MsSVT"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/dscdyc/MsSVT?style=social"> </a>

* [RBGNet: Ray-based grouping for 3d object detection](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_RBGNet_Ray-Based_Grouping_for_3D_Object_Detection_CVPR_2022_paper.pdf).\\
**Haiyang Wang**, Shaoshuai Shi<sup>†</sup>, Ze Yang, Rongyao Fang, Qi Qian, Hongsheng Li, Bernt Schiele, Liwei Wang<sup>†</sup>. In [**CVPR 2022**](https://cvpr2022.thecvf.com/). \[[Code](https://github.com/Haiyang-W/RBGNet)\]  &nbsp;<a href="https://github.com/Haiyang-W/RBGNet"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/Haiyang-W/RBGNet?style=social"> </a>

* [Non-convex Distributionally Robust Optimization:Non-asymptotic Analysis](https://proceedings.neurips.cc/paper/2021/file/164bf317ea19ccfd9e97853edc2389f4-Supplemental.pdf).\\
Jikai Jin\*, Bohang Zhang\*, **Haiyang Wang**, Liwei Wang. In [**NeurIPS 2021**](https://neurips.cc/Conferences/2021).

* [Explicit shape encoding for real-time instance segmentation](https://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_Explicit_Shape_Encoding_for_Real-Time_Instance_Segmentation_ICCV_2019_paper.pdf).\\
Wenqiang Xu\*, **Haiyang Wang\***, Fubo Qi, Cewu Lu. In [**ICCV 2019**](https://iccv2019.thecvf.com/). \[[Code](https://github.com/WenqiangX/ese_seg)\] &nbsp;<a href="https://github.com/WenqiangX/ese_seg"><img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/WenqiangX/ese_seg?style=social"> </a>

## Selected Awards
* **National Scholarship (博士国家奖学金)**, 2022-2023. Awarded annually to top 1 student in all grades of Center for Data Science, Peking University. 
* **National Scholarship (博士国家奖学金)**, 2021-2022. Awarded annually to top 1 student in all grades of Center for Data Science, Peking University. 
* **Excellent Student at Shanghai Jiaotong University**, 2020.  Top 5% of graduated students.

## Experiences
* Visiting Student at [MPI-INF D2](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning). Advisor: [Bernt Schiele](https://scholar.google.com/citations?user=z76PBfYAAAAJ&hl=en&oi=ao) (2023.12 - 2024.12)
* Internship at [SenseTime](https://www.sensetime.com/en). Advisor: [Jifeng Dai](https://scholar.google.com/citations?user=SH_-B_AAAAAJ&hl=en&oi=ao) and [Wenguan Wang](https://scholar.google.com/citations?user=CqAQQkgAAAAJ&hl=zh-CN) (2020.07 - 2021.07)
* Visiting Student at [ScAi-UCLA](https://scai.cs.ucla.edu/). Advisor: [Wei Wang](https://scholar.google.de/citations?user=UedS9LQAAAAJ&hl=en) (2019.09 - 2020.04)
* Undergraduate at [MVIG-SJTU](https://www.mvig.org/). Advisor: [Cewu Lu](https://scholar.google.com/citations?user=QZVQEWAAAAAJ&hl=en) (2018.02 - 2019.05)

## Invited Talks
* TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters.
  * 2024.10. Hosted by [BAAI(智源)](https://event.baai.ac.cn). [[Video](https://event.baai.ac.cn/activities/858)]
  * 2024.11. Hosted by [Huawei Noah's Ark Lab](https://noahlab.com.hk/#/home). 
  * 2024.10. Hosted by [Google VIA Center](https://www.mpg.de/18787770/research-center-visual-computing-interaction-artificial-intelligence)
* GiT: Towards Generalist Vision Transformer through Universal Language Interface.
  * 2024.4. Hosted by [Huawei Technologies Co., Ltd](https://www.huawei.com/cn/corporate-information).
  * 2024.4. Hosted by [Google VIA Center](https://www.mpg.de/18787770/research-center-visual-computing-interaction-artificial-intelligence)
  * 2024.1. Hosted by [Prof Bernt Schiele](https://scholar.google.com/citations?user=z76PBfYAAAAJ&hl=en) in [MPI-INF](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning)
* A Unified and Efficient Multi-Modal Transformer for Bird’s-Eye-View Representation.
  * 2023.12. Hosted by [TechBeat](https://www.techbeat.net/). \[[Video](https://www.techbeat.net/talk-info?id=838)\] 
  * 2023.11. Hosted by [智东西](https://course.zhidx.com/). \[[Video](https://course.zhidx.com/c/OGI4M2UzNDJkYWNiNGRkZWQyODM=)\]
  * 2023.11. Hosted by [自动驾驶之心](https://www.zdjszx.com/).
  * 2023.6. Hosted by [Huawei Noah's Ark Lab](https://noahlab.com.hk/#/home). 

## Academic Services
* Reviewer for NeurIPS’21, CVPR’22, ECCV’22, ICML’22, NeurIPS’22, CVPR’23, ICML’23, ICCV’23, NeurIPS’23, IROS’23, CVPR'24, ICML'24, NeurIPS’24 (**<font color=red>Top Reviewer</font>**), ICLR'25, ICLR'26
